\chapter{Introduction and Objectives of the Work}
\label{chap:intro_eng}

The volume of activity carried out by High-Performance Computing (HPC) centres has led to an exponential increase in the amount of data generated and monitored within research institutions and energy infrastructures, for example, data collected by sensors inside the rooms housing the servers.  
This energy and environmental consumption data is a valuable source of information but also potentially sensitive: by analysing power and temperature variations, it is possible to indirectly infer information about system activity, such as workload intensity or peak usage times.  
Access to and sharing of this data are therefore limited by confidentiality and security constraints.  
In this context, the \textbf{generation of realistic synthetic time series} represents a promising solution to reconcile two often conflicting requirements: on the one hand, the need to conduct advanced analyses and develop predictive models; on the other, the protection of privacy and operational security within data centres.  
Creating synthetic data consistent with real data makes it possible to train and validate artificial intelligence models, experiment with optimisation algorithms, and evaluate energy efficiency strategies without exposing confidential information.

\section*{Objective of the Work}

The objective of this work was to \textbf{produce synthetic time series of energy consumption that are realistic and consistent with the real data of the HPC centre in Turin}, using generative artificial intelligence models based on diffusion processes.  
In particular, the project is part of the broader field of research on conditional generation of time series data, in which synthetic sequences are created by taking into account auxiliary variables --- such as rack type, date, time, or other operating conditions --- in order to preserve the original patterns and correlations.

\section*{Approach Followed}

To address the problem, I designed and implemented a complete analysis and modelling workflow, consisting of four main phases:

\begin{enumerate}
  \item \textbf{Data cleaning and normalisation}: collection and cleaning of energy data from InfluxDB, with verification of temporal consistency, removal of outliers, and standardisation of sampling frequencies;
  \item \textbf{Exploratory analysis and clustering}: study of correlations between electrical quantities and identification of recurring operating regimes using \emph{k-Means} and \emph{PCA} techniques;
  \item \textbf{Generative training with \emph{WaveStitch}}: application and adaptation of the \emph{WaveStitch} diffusion model~\cite{wavestitch} for the conditional generation of time series in the HPC energy domain;
  \item \textbf{Real vs synth evaluation and comparison}: analysis of synthetic sequences generated through numerical (\emph{MSE}) and structural (\emph{ACD} and \emph{xCorrDiff}) metrics to verify fidelity and statistical consistency with the original data.
\end{enumerate}
This approach made it possible to build a replicable and scalable workflow, capable of integrating \emph{data preprocessing}, \emph{unsupervised learning}, and \emph{generative artificial intelligence} techniques within a single application framework.

\section*{Main Results}

The results obtained show that, once properly calibrated, the \emph{WaveStitch} model is capable of generating synthetic sequences that are \textbf{consistent, realistic, and statistically aligned} with the real time series of the HPC centre.  
Structural metrics (\emph{Autocorrelation Difference} and \emph{Cross-feature Correlation Difference}) demonstrated high fidelity of temporal and multivariate patterns, while numerical measures (\emph{MSE}) confirmed good point-by-point alignment between real and synthetic traces.
Overall, this work demonstrated how diffusion models can serve as a valid alternative to traditional approaches for data generation and imputation --- in this case, energy data --- contributing to the development of new strategies for \textbf{modelling, simulation, and protection of sensitive data} across a wide range of applications.